# Malware-URL-Lookup-Exercise
Note:
I am not familiar with the python flask framework or other python coding and naming conventions, 
but this repo has implemented functionality required by the write-up.

File:
main.py: including the function that can response a specific api request
/service/DB/simple_db_manager.py: a db manager class that emulates db operations for malware urls
/test/DB/test_simple_db_manager.py: a test class where I write unit test cases for functions in simple_db_manager.py

Install:
1. Clone the repo
2. cd to the working directory
3. Install package by running the command 'pip install -r requirements.txt'

API info:
Request: /v1/urlinfo/{resource_url_with_query_string}
Response: if the input url is not contained in the db that saves malware urls, return {"safe": true}.
          Otherwise, return {"safe": false}

Test the service:
1. Start the python flask server by running the command 'python main.py'.
    main.py will automatically populate the in-memory db. It will save urls google.com, linkedin.com, bestbuy.com, and walmart.com.
2. Open any browser
3. Try a few urls:
    a. localhost:8080/v1/urlinfo/google.com
    b. localhost:8080/v1/urlinfo/meta.com
    c. localhost:8080/v1/urlinfo/

Q&A:
1. The size of the URL list could grow infinitely, how might you scale this beyond the
memory capacity of the system? Bonus if you implement this.
Ans: I implemented a very simple sharding solution in main.py. In short, the idea is that we have a lot of databases
and we want to partition all the malware data. Each of the database will only hold a part of malware data. In our simple 
example, we have db1, db2, and db3. All the query of urls that start with letter a to i will go to db1, with letter j to q will
go to db2, the remaining will go to db3. By using such sharding technique, the service can distribute data with large size into
so many databases, so the URL list may grow "infinitely".

2. The number of requests may exceed the capacity of this system, how might you solve
that? Bonus if you implement this.
Ans: The solution of previous answer can actually solve the problem here. By using the sharding algorithm, all requests
of url starting with letter a to i will go to db1, starting with letter j to q will go to db2, and so on. Another solution
is to have more replicated databases based on your needs, so the get request will access those replicas to handle read operations. 
The third option is to use caching technique such as Redis. The get request's response can be saved into Redis. In the 
future, if we receive the same api request, Redis can give us an answer using really fast speed.

3. What are some strategies you might use to update the service with new URLs? Updates
may be as many as 5000 URLs a day with updates arriving every 10 minutes.
By doing the math, we have 35 update requests every 10 mins, which is not that much from my perspective. I don't think 
we need to use some strategies to handle the easy workload. If in the future the update request increases dramatically,
the sharding solution from the first question should work here.

